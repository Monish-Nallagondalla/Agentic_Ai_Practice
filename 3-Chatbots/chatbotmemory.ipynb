{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['GROQ_API_KEY'] = os.getenv('GROQ_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "model = ChatGroq(model='gemma2-9b-it')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Hello! 👋\\n\\nHow can I help you today? 😊\\n' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 11, 'total_tokens': 26, 'completion_time': 0.027272727, 'prompt_time': 0.001898526, 'queue_time': 0.014598752000000001, 'total_time': 0.029171253}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None} id='run-1082b236-0883-4d65-9145-2c43d30186e3-0' usage_metadata={'input_tokens': 11, 'output_tokens': 15, 'total_tokens': 26}\n"
     ]
    }
   ],
   "source": [
    "result = model.invoke('hello')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hi Monish, it's nice to meet you!\\n\\nThat's awesome that you're a data scientist. What kind of work do you typically do?  \\n\\nI'm always interested in learning more about how people use data to solve problems and gain insights. 😊  \\n\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 62, 'prompt_tokens': 19, 'total_tokens': 81, 'completion_time': 0.112727273, 'prompt_time': 0.001893786, 'queue_time': 0.014826264, 'total_time': 0.114621059}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-e3b50e71-6337-453f-93a9-79447658832f-0', usage_metadata={'input_tokens': 19, 'output_tokens': 62, 'total_tokens': 81})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "model.invoke([HumanMessage(content='Hi Im monish , I am data scientist ')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"You are Monish, a data scientist!  \\n\\nIs there anything else you'd like to know about yourself or discuss?  Perhaps you'd like to talk about your current project, or maybe you have a question about a data science concept?  I'm here to chat. 😊 \\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 64, 'prompt_tokens': 103, 'total_tokens': 167, 'completion_time': 0.116363636, 'prompt_time': 0.005468415, 'queue_time': 0.017820344, 'total_time': 0.121832051}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-87910aae-009b-4938-9508-9438cb8876c0-0', usage_metadata={'input_tokens': 103, 'output_tokens': 64, 'total_tokens': 167})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "model.invoke([\n",
    "    HumanMessage(content='Hi Im monish , I am data scientist '),\n",
    "    AIMessage(content=\"Hi Monish, it's nice to meet you! 👋 \\n\\nIt's great that you're a data scientist.  What kind of work do you do? \\n\\nAre you working on any exciting projects at the moment? I'm always interested in learning more about how data science is being used in different fields. 😊\\n\"),\n",
    "    HumanMessage(content='who am i?')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory \n",
    "\n",
    "\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id:str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "\n",
    "    return store[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableWithMessageHistory(bound=RunnableBinding(bound=RunnableBinding(bound=RunnableLambda(_enter_history), kwargs={}, config={'run_name': 'load_history'}, config_factories=[])\n",
       "| RunnableBinding(bound=RunnableLambda(_call_runnable_sync), kwargs={}, config={'run_name': 'check_sync_or_async'}, config_factories=[]), kwargs={}, config={'run_name': 'RunnableWithMessageHistory'}, config_factories=[]), kwargs={}, config={}, config_factories=[], get_session_history=<function get_session_history at 0x000001EE018347C0>, history_factory_config=[ConfigurableFieldSpec(id='session_id', annotation=<class 'str'>, name='Session ID', description='Unique identifier for a session.', default='', is_shared=True, dependencies=None)])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history = RunnableWithMessageHistory(model,get_session_history)\n",
    "with_message_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hi Monish, it's nice to meet you! How can I help you today?\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 14, 'total_tokens': 36, 'completion_time': 0.04, 'prompt_time': 0.001890155, 'queue_time': 0.014424944, 'total_time': 0.041890155}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-e4c700c0-055a-425d-a348-275f9d2097e5-0', usage_metadata={'input_tokens': 14, 'output_tokens': 22, 'total_tokens': 36})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {'configurable':{'session_id':'Moni'}}\n",
    "\n",
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content='My name is Monish')],\n",
    "    config=config\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is Monish.  \\n\\nI remembered from our earlier conversation. 😊  Is there anything else I can help you with?\\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 30, 'prompt_tokens': 48, 'total_tokens': 78, 'completion_time': 0.054545455, 'prompt_time': 0.002738033, 'queue_time': 0.018097655, 'total_time': 0.057283488}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-ab3f1ea7-dee4-42a3-b5a1-f7b4867de11b-0', usage_metadata={'input_tokens': 48, 'output_tokens': 30, 'total_tokens': 78})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content='what is my name')],\n",
    "    config=config\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InMemoryChatMessageHistory(messages=[HumanMessage(content='My name is Monish', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hi Monish, it's nice to meet you! How can I help you today?\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 14, 'total_tokens': 36, 'completion_time': 0.04, 'prompt_time': 0.001890155, 'queue_time': 0.014424944, 'total_time': 0.041890155}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-e4c700c0-055a-425d-a348-275f9d2097e5-0', usage_metadata={'input_tokens': 14, 'output_tokens': 22, 'total_tokens': 36}), HumanMessage(content='what is my name', additional_kwargs={}, response_metadata={}), AIMessage(content='Your name is Monish.  \\n\\nI remembered from our earlier conversation. 😊  Is there anything else I can help you with?\\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 30, 'prompt_tokens': 48, 'total_tokens': 78, 'completion_time': 0.054545455, 'prompt_time': 0.002738033, 'queue_time': 0.018097655, 'total_time': 0.057283488}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-ab3f1ea7-dee4-42a3-b5a1-f7b4867de11b-0', usage_metadata={'input_tokens': 48, 'output_tokens': 30, 'total_tokens': 78})])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_session_history('Moni')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"You said at the beginning that you have an iPhone 12!  \\n\\nIs there something else you'd like to know about your phone?  Perhaps the model name (like iPhone 12 Pro or iPhone 12 mini)?  Let me know! 😊 \\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 59, 'prompt_tokens': 174, 'total_tokens': 233, 'completion_time': 0.107272727, 'prompt_time': 0.015348539, 'queue_time': 0.016253799, 'total_time': 0.122621266}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-2399ca98-e386-4241-9d74-9e16a53fb33e-0', usage_metadata={'input_tokens': 174, 'output_tokens': 59, 'total_tokens': 233})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config2 = {'configurable':{'session_id':'abc'}}\n",
    "\n",
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content='i have a phone 12')],\n",
    "    config=config2\n",
    ")\n",
    "\n",
    "\n",
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content='which iphone do i have')],\n",
    "    config=config2  \n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Template "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['messages'], input_types={'messages': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x000001EE7FBC36A0>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='you are a helpful assistant,answer all questions in your best ability '), additional_kwargs={}), MessagesPlaceholder(variable_name='messages')])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate,MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system','you are a helpful assistant,answer all questions in your best ability '),\n",
    "        MessagesPlaceholder(variable_name='messages')\n",
    "    ]\n",
    ")\n",
    "\n",
    "prompt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello Monish! 👋\\n\\nIt's nice to meet you. \\n\\nWhat can I do for you today? 😊  I'm ready to answer your questions and help in any way I can.  \\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 46, 'prompt_tokens': 28, 'total_tokens': 74, 'completion_time': 0.083636364, 'prompt_time': 0.002547803, 'queue_time': 0.031317122999999995, 'total_time': 0.086184167}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-4f1e5630-c429-4dba-a700-2b41e5430dfa-0', usage_metadata={'input_tokens': 28, 'output_tokens': 46, 'total_tokens': 74})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | model \n",
    "\n",
    "chain.invoke({'messages':[HumanMessage(content='Hi i am Monish')]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello Moni!  \\n\\nIt's nice to meet you.  I'm ready to help with any questions you have about your iPhone 13 Pro or anything else.  \\n\\nWhat can I do for you today? 😊 \\n\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 53, 'prompt_tokens': 35, 'total_tokens': 88, 'completion_time': 0.096363636, 'prompt_time': 0.002358914, 'queue_time': 0.016031034, 'total_time': 0.09872255}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-250fb9e9-88d7-4f96-9e6f-8f087fd6f9f0-0', usage_metadata={'input_tokens': 35, 'output_tokens': 53, 'total_tokens': 88})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history = RunnableWithMessageHistory(chain,get_session_history)\n",
    "\n",
    "config3 = {'configurable':{'session_id':'bca'}}\n",
    "\n",
    "response = with_message_history.invoke([\n",
    "    HumanMessage(content='my name is Moni i use iphone 13 pro')]\n",
    "    , config= config3\n",
    "\n",
    "\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = response = with_message_history.invoke([\n",
    "    HumanMessage(content='what phone do i have ?')]\n",
    "    , config= config3\n",
    "\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='You have an iPhone 13 Pro!  You mentioned it at the beginning.  😉 \\n\\n\\nLet me know if you have any other questions about it. I can tell you about its features, help you find settings, or even offer some tips and tricks.\\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 57, 'prompt_tokens': 101, 'total_tokens': 158, 'completion_time': 0.103636364, 'prompt_time': 0.005553426, 'queue_time': 0.019950741, 'total_time': 0.10918979}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-a098065e-e35c-4f83-96d1-323894b079af-0', usage_metadata={'input_tokens': 101, 'output_tokens': 57, 'total_tokens': 158})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system',\n",
    "         'you are helful assistant . answer all questions in {language}')\n",
    "    , MessagesPlaceholder(variable_name = 'messages'),]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"That's great! I'm happy to help.  \\n\\nI know Bangalore is a vibrant city with a lot to offer. Is there anything specific you'd like to know or discuss about Bangalore, or anything else? I'm here for you! 😊 \\n\\nFor example, I can help you with:\\n\\n* **Local recommendations:**  Restaurants, cafes, places to visit, etc.\\n* **Information about Bangalore:** History, culture, weather, transportation, etc.\\n* **General knowledge:**  Answering your questions on a wide range of topics.\\n* **Creative tasks:**  Writing stories, poems, or even helping you brainstorm ideas.\\n\\n\\nJust let me know how I can be of assistance!  \\n\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 153, 'prompt_tokens': 28, 'total_tokens': 181, 'completion_time': 0.278181818, 'prompt_time': 0.004372879, 'queue_time': 0.017830620000000002, 'total_time': 0.282554697}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-f9a76982-0057-414a-92bf-af0639649703-0', usage_metadata={'input_tokens': 28, 'output_tokens': 153, 'total_tokens': 181})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rresponse = chain.invoke({'messages':[HumanMessage(content='I am from bangalore')],'language':'Kannada'})\n",
    "rresponse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Managing the Conversation History\n",
    "One important concept to understand when building chatbots is how to manage conversation history. If left unmanaged, the list of messages will grow unbounded and potentially overflow the context window of the LLM. Therefore, it is important to add a step that limits the size of the messages you are passing in.\n",
    "'trim_messages' helper to reduce how many messages we're sending to the model. The trimmer allows us to specify how many tokens we want to keep, along with other parameters like if we want to always keep the system message and whether to allow partial messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"you're a good assistant\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content=\"hi! I'm bob\", additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='hi!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='I dont like vanilla ice cream', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='nice', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='whats 2 + 2', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='4', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='thanks', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='no problem!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='having fun?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='yes!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Trimming \n",
    "from langchain_core.messages import SystemMessage , trim_messages\n",
    "trimmer = trim_messages(\n",
    "    max_tokens=55,\n",
    "    strategy = 'last',\n",
    "    token_counter = model,\n",
    "    include_system = True,\n",
    "    allow_partial = False,\n",
    "    start_on = 'human'\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"you're a good assistant\"),\n",
    "    HumanMessage(content=\"hi! I'm bob\"),\n",
    "    AIMessage(content=\"hi!\"),\n",
    "    HumanMessage(content=\"I dont like vanilla ice cream\"),\n",
    "    AIMessage(content=\"nice\"),\n",
    "    HumanMessage(content=\"whats 2 + 2\"),\n",
    "    AIMessage(content=\"4\"),\n",
    "    HumanMessage(content=\"thanks\"),\n",
    "    AIMessage(content=\"no problem!\"),\n",
    "    HumanMessage(content=\"having fun?\"),\n",
    "    AIMessage(content=\"yes!\")\n",
    "]\n",
    "trimmer.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableAssign(mapper={\n",
       "  messages: RunnableLambda(itemgetter('messages'))\n",
       "            | RunnableLambda(...)\n",
       "})\n",
       "| ChatPromptTemplate(input_variables=['language', 'messages'], input_types={'messages': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x000001EE7FBC36A0>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['language'], input_types={}, partial_variables={}, template='you are helful assistant . answer all questions in {language}'), additional_kwargs={}), MessagesPlaceholder(variable_name='messages')])\n",
       "| ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000001EE01968200>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000001EE01969340>, model_name='gemma2-9b-it', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "chain = (RunnablePassthrough.assign(messages=itemgetter('messages')| trimmer)| prompt | model)\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You said you don't like vanilla ice cream!  \\n\\nDo you want to tell me what flavors you *do* like? 😊🍦  \\n\\n\\n\\n\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke(\n",
    "    {'messages':messages + [HumanMessage(content='what icecream do i like?')],\n",
    "                            'language':'English'}\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You asked \"whats 2 + 2\" \\n\\n\\n\\n'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke(\n",
    "    {\n",
    "        \"messages\": messages + [HumanMessage(content=\"what math problem did i ask\")],\n",
    "        \"language\": \"English\",\n",
    "    }\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
